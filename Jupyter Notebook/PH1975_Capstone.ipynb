{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee93c90",
   "metadata": {},
   "source": [
    "## Readme\n",
    "This is a Readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a606a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def WebScraper():\n",
    "    import sys\n",
    "    import gc\n",
    "    from datetime import datetime\n",
    "    import requests as rq\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    \n",
    "    #Clear memory\n",
    "    gc.collect()\n",
    "    #Create the URL Terms\n",
    "    base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "    size = \"200\"\n",
    "    term = 'schizophrenia'\n",
    "    date1 = '2000%2F1%2F1'\n",
    "    date2 = '2020%2F11%2F15'\n",
    "    pmformat = 'abstract'\n",
    "    #Pulls from the first 50 pages\n",
    "    pages = list(range(1,1))\n",
    "    #Create CSV File\n",
    "    #Timestamp\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"{term}_{date_time}.csv\"\n",
    "    #Creates a new CSV file w/ keyword+timestamp as the filename\n",
    "    file = open(filename,'w')\n",
    "    file.write('Paper Title,Publication Time,Abstract\\n')\n",
    "    \n",
    "    #Iterate for page numbers\n",
    "    for page in pages:\n",
    "        \n",
    "        #Create URL\n",
    "        full_url = f\"{base_url}?term={term}&filter=dates.{date1}-{date2}&format={pmformat}&tsize={size}&page={page}\"\n",
    "\n",
    "        #Construct the BS object\n",
    "        page = rq.get(full_url)\n",
    "        soup = bs(page.content,\"html.parser\")\n",
    "        \n",
    "        #For loop for results on each page\n",
    "        for result in soup.find_all(\"div\", class_=\"results-article\"):\n",
    "            #~~~~Article name\n",
    "            #Find article name in HTML tree\n",
    "            article_name= result.find(\"h1\",class_=\"heading-title\")\n",
    "            #Scrape text\n",
    "            article_string = article_name.get_text()\n",
    "            article_string = article_string.replace(',','')\n",
    "            #Write using f-string\n",
    "            file.write(f\"{(article_string.strip())},\")\n",
    "\n",
    "            '''#~~~~Author List, loop for multiple authors\n",
    "            author_concat = ''\n",
    "            for author in result.find_all(\"span\",class_=\"author-list-item\"):\n",
    "                author_name = author.a.get_text()\n",
    "                author_name = author_name.replace(',',';')\n",
    "                author_concat = f\"{author_concat}{author_name} ;\"\n",
    "            file.write(f\"{author_concat},\")'''\n",
    "            #~~~~Publication Time\n",
    "            #Find publication time in HTML tree\n",
    "            pub_time = result.find(\"span\",class_='cit')\n",
    "            #Scrape text\n",
    "            pub_string = pub_time.get_text()\n",
    "            #Clean up date after ;\n",
    "            pub_string = pub_string.split(';',1)[0]\n",
    "            pub_string = pub_string.replace(',','')\n",
    "            #Write using f-string\n",
    "            file.write(f\"{pub_string},\")\n",
    "\n",
    "            #~~~~Abstract\n",
    "            #Find abstract in HTML tree\n",
    "            abstract = result.find(\"div\",class_='abstract-content selected')\n",
    "            #Scrape text\n",
    "            try:\n",
    "                abstract_string = abstract.p.get_text()\n",
    "            except AttributeError:\n",
    "                abstract_string = 'No abstract found?'\n",
    "            #Remove commas because CSV goes weird (maybe a better way to do this)\n",
    "            abstract_string = abstract_string.replace(\",\",\"\")\n",
    "            abstract_string = abstract_string.replace(\":\",\"\")\n",
    "            abstract_string = abstract_string.replace(\"\\n\",\"\")\n",
    "            abstract_string = abstract_string.strip()\n",
    "            #Write using f-string\n",
    "            try:\n",
    "                file.write(f\"{abstract_string}\\n\")\n",
    "            except:\n",
    "                abstract_string = abstract_string.replace(\",\",\"\")\n",
    "                abstract_string = abstract_string.replace(\":\",\"\")\n",
    "                abstract_string = abstract_string.replace(\"\\n\",\"\")\n",
    "                abstract_string = abstract_string.strip()\n",
    "                abstract_string = abstract_string.encode(sys.stdout.encoding, errors='replace')\n",
    "                file.write(f\"{abstract_string}\\n\")\n",
    "    #Close file\n",
    "    file.close()\n",
    "    #Complete status\n",
    "    print('done')\n",
    "WebScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13135d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
