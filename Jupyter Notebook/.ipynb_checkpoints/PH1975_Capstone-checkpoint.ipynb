{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee93c90",
   "metadata": {},
   "source": [
    "## Scraper w/o BioPython\n",
    "This is a Readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a606a4d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (1477821058.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    def WebScraper():\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "def GetURL():\n",
    "    #stuff\n",
    "\n",
    "def WebScraper():\n",
    "    import sys\n",
    "    import gc\n",
    "    from datetime import datetime\n",
    "    import requests as rq\n",
    "    from requests_html import AsyncHTMLSession\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    from regex import regex as re\n",
    "    \n",
    "    #Clear memory\n",
    "    gc.collect()\n",
    "    #Create the URL Terms\n",
    "    base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "    size = \"200\"\n",
    "    term = 'schizophrenia'\n",
    "    date1 = '2000%2F1%2F1'\n",
    "    date2 = '2020%2F11%2F15'\n",
    "    pmformat = 'abstract'\n",
    "    #Pulls from the first 50 pages\n",
    "    pages = list(range(1,3))\n",
    "    #Create CSV File\n",
    "    #Timestamp\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"{term}_{date_time}.csv\"\n",
    "    #Creates a new CSV file w/ keyword+timestamp as the filename\n",
    "    file = open(filename,'w')\n",
    "    file.write('Paper Title,Publication Time,Abstract\\n')\n",
    "    \n",
    "    #Iterate for page numbers\n",
    "    for page in pages:\n",
    "        \n",
    "        #Create URL\n",
    "        full_url = f\"{base_url}?term={term}&filter=dates.{date1}-{date2}&format={pmformat}&tsize={size}&page={page}\"\n",
    "        print(full_url)\n",
    "        #Wait for page response\n",
    "        asession = AsyncHTMLSession()\n",
    "        response = await asession.get(full_url)\n",
    "        await response.html.arender()\n",
    "        #Construct the BS object\n",
    "        page = rq.get(full_url)\n",
    "        soup = bs(page.content,\"html.parser\")\n",
    "        \n",
    "        #For loop for results on each page\n",
    "        for result in soup.find_all(\"div\", class_=\"results-article\"):\n",
    "            #~~~~Article name\n",
    "            #Find article name in HTML tree\n",
    "            article_name= result.find(\"h1\",class_=\"heading-title\")\n",
    "            #Scrape text\n",
    "            article_string = article_name.a.get_text()\n",
    "            article_string = article_string.replace(',','')\n",
    "            #Write using f-string\n",
    "            file.write(f\"{(article_string.strip())},\")\n",
    "\n",
    "            #~~~~Author List, loop for multiple authors\n",
    "            author_concat = ''\n",
    "            for author in result.find_all(\"span\",class_=\"authors-list-item\"):\n",
    "                print(author.a.get_text())\n",
    "                #author_name = author_name.replace(',',';')\n",
    "                #author_concat = f\"{author_concat}{author_name} ;\"\n",
    "            print(f\"{author_concat},\")\n",
    "            #~~~~Publication Time\n",
    "            #Find publication time in HTML tree\n",
    "            pub_time = result.find(\"span\",class_='cit')\n",
    "            #Scrape text\n",
    "            pub_string = pub_time.get_text()\n",
    "            #Clean up date after ;\n",
    "            pub_string = pub_string.split(';',1)[0]\n",
    "            pub_string = pub_string.replace(',','')\n",
    "            #Write using f-string\n",
    "            file.write(f\"{pub_string},\")\n",
    "\n",
    "            #~~~~Abstract\n",
    "            #Find abstract in HTML tree\n",
    "            abstract = result.find(\"div\",class_='abstract-content selected')\n",
    "            #Scrape text\n",
    "            try:\n",
    "                abstract_string = abstract.p.get_text()\n",
    "            except AttributeError:\n",
    "                abstract_string = 'No abstract found?'\n",
    "            #Remove commas because CSV goes weird (maybe a better way to do this)\n",
    "            abstract_string = abstract_string.replace(\",\",\"\")\n",
    "            #Remove colons\n",
    "            abstract_string = abstract_string.replace(\":\",\"\")\n",
    "            #Remove line feed\n",
    "            abstract_string = abstract_string.replace(\"\\n\",\"\")\n",
    "            #Remove padded white space\n",
    "            abstract_string = abstract_string.strip()\n",
    "            #Remove additional white space with RegEx\n",
    "            abstract_string = re.sub(\"\\s\\s+\",\" \",abstract_string)\n",
    "            #Write using f-string\n",
    "            try:\n",
    "                file.write(f\"{abstract_string}\\n\")\n",
    "            except:\n",
    "                abstract_string = abstract_string.replace(\",\",\"\")\n",
    "                abstract_string = abstract_string.replace(\":\",\"\")\n",
    "                abstract_string = abstract_string.replace(\"\\n\",\"\")\n",
    "                abstract_string = abstract_string.strip()\n",
    "                abstract_string = abstract_string.encode(sys.stdout.encoding, errors='replace')\n",
    "                abstract_string = re.sub(\"\\s\\s+\",\" \",abstract_string)\n",
    "                file.write(f\"{abstract_string}\\n\")\n",
    "    #Close file\n",
    "    file.close()\n",
    "    #Complete status\n",
    "    print('done')\n",
    "WebScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5edf9",
   "metadata": {},
   "source": [
    "## Scraper w/ BioPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ec3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 30/500 [00:14<03:39,  2.14it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m         entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamelist\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpub_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabstract\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(entry)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mBioWebScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [1], line 52\u001b[0m, in \u001b[0;36mBioWebScraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m namelist \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     51\u001b[0m authors \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorList\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m \u001b[43mauthors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m         lastname \u001b[38;5;241m=\u001b[39m author\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLastName\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "def BioWebScraper():\n",
    "    import sys\n",
    "    import gc\n",
    "    from datetime import datetime\n",
    "    import requests as rq\n",
    "    from requests_html import AsyncHTMLSession\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    from regex import regex as re\n",
    "    import Bio\n",
    "    from Bio import Entrez\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    #Clear memory\n",
    "    gc.collect()\n",
    "    \n",
    "    #Create Field Terms\n",
    "    dbase = 'pubmed'\n",
    "    mesh_term = 'schizophrenia'\n",
    "    num_articles = '500' #Number of articles to pull\n",
    "    begin = '2000/01/01'\n",
    "    end = '2022/11/15'\n",
    "    \n",
    "    #TimeStamp for file creation\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    #Creates a new CSV file w/ keyword+timestamp as the filename\n",
    "    filename = f\"{mesh_term}_{date_time}.csv\"\n",
    "    file = open(filename,'w')\n",
    "    file.write('Paper Title,Authors,Publication Date,Abstract\\n')\n",
    "    \n",
    "    #Connect to PubMed\n",
    "    Entrez.email = \"acking1187@gmail.com\"\n",
    "    handle = Entrez.esearch(db=dbase, term=mesh_term, retmax=num_articles,mindate=begin,maxdate=end)\n",
    "    records = Entrez.read(handle)\n",
    "    \n",
    "    #Parse XML into the CSV\n",
    "    i=0\n",
    "    for record in tqdm(records['IdList']):\n",
    "        entry = Entrez.efetch(db='pubmed', id=record,retmode='xml')\n",
    "        soup = bs(entry,\"xml\")\n",
    "        #print(soup.prettify())\n",
    "        \n",
    "        #Get Paper Title\n",
    "        article_title = soup.find(\"ArticleTitle\").get_text().replace(\",\",\"\")\n",
    "        article_title = article_title.encode(sys.stdout.encoding, errors='replace')\n",
    "        #print(article_title)\n",
    "        \n",
    "        #Get Authors\n",
    "        namelist = ''\n",
    "        authors = soup.find(\"AuthorList\")\n",
    "        for author in authors.find_all(\"Author\"):\n",
    "            try:\n",
    "                lastname = author.find('LastName').get_text()\n",
    "            except:\n",
    "                lastname = ''\n",
    "            try:\n",
    "                firstname = author.find('ForeName').get_text()\n",
    "            except: \n",
    "                firstname = ''\n",
    "            if firstname==\"\" and lastname==\"\":\n",
    "                continue\n",
    "            name = f\"{lastname} {firstname}\"\n",
    "            namelist = f\"{namelist}{name}; \"\n",
    "        namelist = namelist.encode(sys.stdout.encoding, errors='replace')\n",
    "        #print(namelist)\n",
    "        \n",
    "        #Get Publication Date\n",
    "        pub_date = soup.find(\"PubDate\").get_text()\n",
    "        pub_date = re.sub(r'(\\d{4})(.{3})(\\d{2})',r'\\1 \\2 \\3',pub_date)\n",
    "        pub_date = pub_date.encode(sys.stdout.encoding, errors='replace')\n",
    "        #print(pub_date)\n",
    "        \n",
    "        #Get Abstract\n",
    "        try:\n",
    "            abstract = soup.find(\"AbstractText\").get_text().replace(\",\",\"\")\n",
    "        except:\n",
    "            abstract = 'No abstract.'\n",
    "        abstract = abstract.encode(sys.stdout.encoding, errors='replace')\n",
    "        #print(abstract)\n",
    "        \n",
    "        #Create CSV Entry\n",
    "        entry = f\"{article_title},{namelist},{pub_date},{abstract}\\n\"\n",
    "        file.write(entry)\n",
    "        \n",
    "BioWebScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac47644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
